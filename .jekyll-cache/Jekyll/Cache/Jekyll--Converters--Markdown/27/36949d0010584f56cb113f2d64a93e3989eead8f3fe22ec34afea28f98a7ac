I"@E<p>Le hashing consistant a été introduit en 1997 par le papier <a href="https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf">Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a> 
de Karger D., Lehman E., Leighton T., Panigrahy R., Levine M., Lewin, D.
Le hashing consistant est aujourd’hui une brique fondatrice et incontournable à beaucoup de technologies dites distribuées. 
Comprendre ce qu’est le hashing consistant permet d’appréhender ces technos et d’en imaginer le fonctionnement.
Ce billet a pour objectif de vulgariser le concept de hashing consistant.</p>

<p>A la fin des années 90, les sites internet populaires commencent à crouler sous les requêtes des internautes.
Chaque document servi est identifié par une requête unique, genre <code class="highlighter-rouge">http://www.altavista.fr/search?q=netscape</code>.
Le gros serveur™ reçoit la requête, fait un calcul coûteux avant de servir un document à l’internaute.
Et quand le serveur reçoit 15000 requêtes en même temps …</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ____                          _ _ 
| __ )  ___   ___  _ __ ___   | | |
|  _ \ / _ \ / _ \| '_ ` _ \  | | |
| |_) | (_) | (_) | | | | | | |_|_|
|____/ \___/ \___/|_| |_| |_| (_|_)
</code></pre></div></div>

<p>On se rend compte que :</p>

<ul>
  <li>certaines url sont plus sollicitées que d’autres et qu’il est dommage de faire le même calcul plusieurs fois</li>
  <li>le gros serveur est taillé pour les calculs et pas pour la distribution de contenu</li>
</ul>

<p>La solution consiste à mettre un serveur de cache en frontal. Le serveur de cache a deux possibilités.
Soit il possède localement le résultat pour l’URL qu’on lui demande (en mémoire ou sur disque), dans ce cas il répond
instantanément, soit il n’a pas le résultat alors il le demande au gros serveur, répond, et le stocke localement.
Avec ça, on a résolu le problème du calcul redondant et déporté la responsabilité de servir du contenu.
Mais quand le serveur de cache reçoit 30000 requêtes en même temps …</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ____                          _ _ 
| __ )  ___   ___  _ __ ___   | | |
|  _ \ / _ \ / _ \| '_ ` _ \  | | |
| |_) | (_) | (_) | | | | | | |_|_|
|____/ \___/ \___/|_| |_| |_| (_|_)
</code></pre></div></div>

<p>Alors on multiplie les serveurs de cache et on ajoute un load balancer. Le load balancer reçoit toutes les URLs et
les répartit aux serveurs de cache. Et voici, le schéma qu’il faut mettre en forme avec PowerPoint :</p>

<p><img src="/images/articles/consistent-hashing/schema.jpe" alt="Image schéma complet" /></p>

<p>L’algorithme magique de répartition d’une URL à un serveur de cache est le suivant :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>serveur-de-cache = hash(URL) modulo nb-de-serveur-de-cache
</code></pre></div></div>

<p>Simplissime ! Pour une URL donnée, on sollicite toujours le même serveur de cache, et on sollicite le moins possible 
le gros serveur™. On a résolu pour de bon le problème de mise à l’échelle (scalabilité). On attend le prix Nobel.</p>

<p>Mais qu’est ce qui ce passe si on débranche l’un des serveurs de cache ? Et bien, <code class="highlighter-rouge">nombre-de-serveur-de-cache</code> est
 décrémenté, et l’algorithme magique ne distribue
plus du tout les URLs aux mêmes serveurs de cache. Cela signifie que pendant un moment, les serveurs de cache sollicitent
systématiquement le gros serveur™ et …</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ____                          _ _ 
| __ )  ___   ___  _ __ ___   | | |
|  _ \ / _ \ / _ \| '_ ` _ \  | | |
| |_) | (_) | (_) | | | | | | |_|_|
|____/ \___/ \___/|_| |_| |_| (_|_)
</code></pre></div></div>

<p>Et c’est à ce moment que je commence à vous parler des algorithmes de hashage consistants, et que je plagie 
<a href="https://fr.wikipedia.org/wiki/Hachage_coh%C3%A9rent">Wikipedia</a>.
La façon la plus simple de saisir le concept est d’utiliser l’allégorie du cercle. Sur un cercle, on peut apposer
n’importe quel serveur de cache par la formule magique :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    angle-du-cercle = hash(serveur-de-cache) modulo 360
</code></pre></div></div>

<p>On peut également apposer n’importe qu’elle URL par la formule magique :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    angle-du-cercle = hash(URL) modulo 360
</code></pre></div></div>

<p>Ce qui pourrait donner un cercle du genre :</p>

<p><img src="/images/articles/consistent-hashing/cercle.jpe" alt="Image les serveurs et les URLs sur un cercle" /></p>

<p>Maintenant mettons-nous à la place du load-balancer. Comment trouver le serveur de cache responsable de l’URL
<code class="highlighter-rouge">http://www.altavista.fr/search?q=netscape</code> ?</p>

<p>1) On calcule la position de l’URL sur le cercle, disons <code class="highlighter-rouge">73°</code></p>

<p>2) Connaissant la position de tous les serveurs de cache sur le cercle, on recherche celui qui est le plus proche l’URL
tout en étant plus loin selon le sens horaire, soit le serveur de cache n°2.</p>

<p>On a donc pour le moment les correspondances suivantes :</p>

<ul>
  <li>Serveur de cache n°1 : <code class="highlighter-rouge">http://www.altavista.fr/search?q=gopher</code></li>
  <li>Serveur de cache n°2 : <code class="highlighter-rouge">http://www.altavista.fr/search?q=netscape</code></li>
  <li>Serveur de cache n°3 : <code class="highlighter-rouge">http://www.altavista.fr/search?q=opera</code> et <code class="highlighter-rouge">http://www.altavista.fr/search?q=mosaic</code></li>
</ul>

<p>Maintenant supprimons le serveur de cache n°1. La distribution des URLs est la suivante :</p>

<ul>
  <li>Serveur de cache n°2 : <code class="highlighter-rouge">http://www.altavista.fr/search?q=gopher</code> et <code class="highlighter-rouge">http://www.altavista.fr/search?q=netscape</code></li>
  <li>Serveur de cache n°3 : <code class="highlighter-rouge">http://www.altavista.fr/search?q=opera</code> et <code class="highlighter-rouge">http://www.altavista.fr/search?q=mosaic</code></li>
</ul>

<p>On constate que <strong>la distribution des URLs précédente a très peu été boulversée</strong>. Seules les URLs dont avait la charge le 
serveur de cache n°1 ont été redistribuées. Le gros serveur™ n’est sollicité que pour la clé <code class="highlighter-rouge">http://www.altavista.fr/search?q=gopher</code>.</p>

<p>Essayons de pousser encore le vice. Comment faire pour que lorsqu’un serveur de cache est supprimé, on ne doive pas 
 solliciter du tout le gros serveur™ ?
Simplement en dupliquant les responsabilités. Par exemple, on peut décider de chercher soit le serveur de cache le plus
proche dans le sans horaire OU dans le sens inverse. Ainsi l’URL <code class="highlighter-rouge">http://www.altavista.fr/search?q=netscape</code> peut
être servie par le serveur de cache n°2 ou le serveur de cache n°1.</p>

<p>On peut aussi décider de répliquer N fois la responsabilité d’une URL, le load-balancer peut utiliser la formule magique :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    angle-du-cercle = hash(URL + (random N)) modulo 360
</code></pre></div></div>

<p>Une URL peut donc se retrouver à N positions différentes sur le cercle, et potentiellement être prise en charge par 
N serveurs de cache différents.</p>

<p>Avec un peu d’imagination, plein d’autres stratégies de réplication peuvent être implémentées.</p>

<p>On s’apperçoit que l’élément crutial de cet algorithme est la distribution des serveurs de ce cache sur le cercle.
 C’est la fonction de hashage qui en est responsable. 
L’algorithme de hashage doit avoir une très bonne distribution. Notre exemple ne comporte que trois serveurs de cache,
mais on peut remarquer la proximité géographique du serveur de cache n°1 et du serveur de cache n°2. Dans l’idéal,
ces trois serveurs de cache se situent à une distance de 120° l’un de l’autre.</p>

<p>Il serait catastrophique d’avoir nos trois serveurs séparés de quelques degrés seulement. La charge serait très mal
 distribuée.</p>

<p>Si vous utilisez le langage Java, je vous déconseille d’utiliser la fonction <code class="highlighter-rouge">Object.hashcode()</code> qui a une très 
mauvaise distribution car <code class="highlighter-rouge">"serveur2".hashCode() = "serveur1".hashCode() + 1</code>.</p>

<p>D’un autre côté, les fonctions de hashage cryptogragraphique, telles que <code class="highlighter-rouge">SHA1</code> ou <code class="highlighter-rouge">MD5</code> bien qu’ayant une très bonne
distribution, avec peu de collisions, sont coûteuses en CPU.</p>

<p>Je conseille d’utiliser des fonctions de hashages plus légères, telle que <a href="https://en.wikipedia.org/wiki/MurmurHash">Murmur</a>,
qui offre une bonne distribution, peu de collisions et est très rapide. Voir aussi cette <a href="http://programmers.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed/145633#145633">réponse sur stackexchange</a>
qui compare différentes fonctions de hashage non cryptographiques.</p>

<p>On ne peut pas se quitter sans essayer d’implémenter un hashage consitant ! Voici quelques
lignes de Clojure qui implémentent très simplement ce que j’ai décrit ci-dessus.</p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="nf">defprotocol</span><span class="w"> </span><span class="n">ConsistentHashRing</span><span class="w">
  </span><span class="p">(</span><span class="nf">add-node</span><span class="w"> </span><span class="p">[</span><span class="n">this</span><span class="w"> </span><span class="nb">node</span><span class="p">]</span><span class="w"> </span><span class="s">"add node to the ring"</span><span class="p">)</span><span class="w">
  </span><span class="p">(</span><span class="nf">remove-node</span><span class="w"> </span><span class="p">[</span><span class="n">this</span><span class="w"> </span><span class="nb">node</span><span class="p">]</span><span class="w"> </span><span class="s">"remove node and its replicas of the ring"</span><span class="p">)</span><span class="w">
  </span><span class="p">(</span><span class="nf">find-node</span><span class="w"> </span><span class="p">[</span><span class="n">this</span><span class="w"> </span><span class="n">data</span><span class="p">]</span><span class="w"> </span><span class="s">"find the node responsible of data"</span><span class="p">))</span><span class="w">

</span><span class="p">(</span><span class="k">defn-</span><span class="w"> </span><span class="n">find-closest-key</span><span class="w"> </span><span class="p">[</span><span class="n">xs</span><span class="w"> </span><span class="n">h</span><span class="p">]</span><span class="w">
  </span><span class="p">(</span><span class="nb">or</span><span class="w"> </span><span class="p">(</span><span class="nb">first</span><span class="w"> </span><span class="p">(</span><span class="nb">drop-while</span><span class="w"> </span><span class="o">#</span><span class="p">(</span><span class="nb">&gt;</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="n">%</span><span class="p">)</span><span class="w"> </span><span class="n">xs</span><span class="p">))</span><span class="w">
      </span><span class="p">(</span><span class="nb">first</span><span class="w"> </span><span class="n">xs</span><span class="p">)))</span><span class="w">

</span><span class="p">(</span><span class="nf">extend-protocol</span><span class="w"> </span><span class="n">ConsistentHashRing</span><span class="w">
  </span><span class="n">PersistentTreeMap</span><span class="w">
  </span><span class="p">(</span><span class="nf">add-node</span><span class="w"> </span><span class="p">[</span><span class="n">this</span><span class="w"> </span><span class="nb">node</span><span class="p">]</span><span class="w"> </span><span class="p">(</span><span class="nb">assoc</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="p">(</span><span class="nf">.hashCode</span><span class="w"> </span><span class="nb">node</span><span class="p">)</span><span class="w"> </span><span class="nb">node</span><span class="p">))</span><span class="w">
  </span><span class="p">(</span><span class="nf">remove-node</span><span class="w"> </span><span class="p">[</span><span class="n">this</span><span class="w"> </span><span class="nb">node</span><span class="p">]</span><span class="w"> </span><span class="p">(</span><span class="nb">dissoc</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="p">(</span><span class="nf">.hashCode</span><span class="w"> </span><span class="nb">node</span><span class="p">)))</span><span class="w">
  </span><span class="p">(</span><span class="nf">find-node</span><span class="w"> </span><span class="p">[</span><span class="n">this</span><span class="w"> </span><span class="n">data</span><span class="p">]</span><span class="w"> </span><span class="p">(</span><span class="nf">this</span><span class="w"> </span><span class="p">(</span><span class="nf">find-closest-key</span><span class="w"> </span><span class="p">(</span><span class="nb">keys</span><span class="w"> </span><span class="n">this</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nf">.hashCode</span><span class="w"> </span><span class="n">data</span><span class="p">)))))</span><span class="w">
</span></code></pre></div></div>

<p>L’implémentation est en Clojure, je suis désolé si ça vous déboussole, mais c’est un langage qui m’est cher et
que je ne peux m’empêcher d’utiliser quand je ne suis pas contraint à ne pas le faire.</p>

<p>Pour cette implémentation, j’ai abstrait un peu les concepts. Un serveur de cache est appelé <code class="highlighter-rouge">node</code>, les URLs sont 
juste de la <code class="highlighter-rouge">data</code>.</p>

<p>Tout d’abord on définit un protocol (en clojure un protocol est une espèce d’interface) appelé <code class="highlighter-rouge">ConsistentHashRing</code>.
 Il y a trois fonctions. <code class="highlighter-rouge">add-node</code> nous permettrait d’ajouter un serveur de cache, 
 <code class="highlighter-rouge">remove-node</code> d’en supprimer un et <code class="highlighter-rouge">find-node</code> de trouver un serveur de cache.</p>

<p>J’ai choisi d’implémenter le protocol sur une classe existante dans Clojure : <code class="highlighter-rouge">PersitentTreeMap</code>. Il s’agit 
 d’une implémentation d’une hashmap qui permet d’itérer sur les clés dans leur ordre naturel. En Java il s’agirait
 de la classe <code class="highlighter-rouge">TreeMap</code>.</p>

<p>L’implémentation de <code class="highlighter-rouge">add-node</code> associe le hashCode de <code class="highlighter-rouge">node</code> à <code class="highlighter-rouge">node</code>. Contrairement à mes conseils j’utilise 
<code class="highlighter-rouge">Object.hashCode()</code> comme fonction de hashage :(</p>

<p>L’implémentation de <code class="highlighter-rouge">remove-node</code> est triviale. Elle supprime la clé dont la valeur est le hashCode de <code class="highlighter-rouge">node</code>.</p>

<p>Enfin, <code class="highlighter-rouge">find-node</code> cherche le <code class="highlighter-rouge">node</code> le plus proche de la <code class="highlighter-rouge">data</code> que l’on cherche.</p>

<p>Cette implémentation est triviale, après tout elle ne fait que 13 lignes ! Mais je trouve qu’elle démontre bien la
 simplicité des concepts du hashing consistant.</p>

<p>Vous pouvez essayer d’implémenter les réplications par dessus. Ou utiliser un autre langage ! Je serais ravi de faire
une relecture de code ;)</p>

<p>Sortons maintenant de notre contexte initial, à savoir le load-balancer et les servers de cache. Que peut-on inventer
grâce au hashage consistant ? Allez, je vous aide un peu :</p>

<ul>
  <li>On peut faire un <a href="https://en.wikipedia.org/wiki/GlusterFS">filesystem distribué</a> tolérant aux pannes</li>
  <li>On peut faire une <a href="http://docs.hazelcast.org/docs/3.5/manual/html/datapartitioning.html">base de donnée mémoire distribuée</a> (j’ai mis le lien qui explique le partitionnement des données)</li>
  <li>On faire une <a href="http://developer.couchbase.com/documentation/server/4.1/concepts/buckets-vbuckets.html">autre base de donnée mémoire distribuée</a> (j’ai mis le lien qui explique ce qu’est un vBucket)</li>
  <li>On peut faire <a href="https://en.wikipedia.org/wiki/Kademlia">index distribué des fichiers qui sont partagés sur Emule</a> (la notion de noeud le plus proche est toutefois plus compliquée !)</li>
</ul>

<p>En fait dès que vous entendez parler d’une technologie qui met les mots : distributed, autoscaling, resiliant, et replication dans
la même phrase, elle utilise sans doute un hashing consistant sous le capot.</p>
:ET